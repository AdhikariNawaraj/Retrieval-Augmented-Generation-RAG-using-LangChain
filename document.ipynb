{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion and Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### document data structures\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exapmle.txt', 'Pages': 1, 'author': 'Nawaraj Adhikari', 'date_created': '2024-01-01'}, page_content='This is the main content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is the main content I am using to create RAG\",\n",
    "    metadata={\"source\": \"exapmle.txt\",\n",
    "              \"Pages\": 1,\n",
    "              \"author\": \"Nawaraj Adhikari\",\n",
    "              'date_created': '2025-01-01'\n",
    "                },\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "samsple_texts = {\n",
    "    \"../data/text_files/python_intro.txt\": \"\"\"Python Programming Introduction\n",
    "    Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and first released in 1991, Python has become one of the most popular programming languages in the world.\n",
    "    Key Features of Python:\n",
    "    1. Readability: Python's syntax is designed to be clear and easy to understand, making it an excellent choice for beginners and experienced developers alike.\n",
    "    2. Versatility: Python can be used for a wide range of applications, including web development, data analysis, artificial intelligence, scientific computing, automation, and more.\n",
    "    3. Extensive Libraries: Python boasts a rich ecosystem of libraries and frameworks, such as NumPy, pandas, Django, Flask, and TensorFlow, which facilitate rapid development and problem-solving.\n",
    "    4. Community Support: Python has a large and active community that contributes to its growth and provides support through forums, tutorials, and documentation.\n",
    "    Applications of Python:\n",
    "    Python is widely used in various fields, including:\n",
    "    - Web Development: Building websites and web applications using frameworks like Django and Flask.\n",
    "    - Data Science: Analyzing and visualizing data with libraries like pandas and Matplotlib.\n",
    "    - Machine Learning: Developing AI models using libraries like TensorFlow and scikit-learn.\n",
    "    - Automation: Writing scripts to automate repetitive tasks and workflows.\n",
    "    - Scientific Computing: Performing complex calculations and simulations in fields like physics and biology.\n",
    "    - Game Development: Creating games using libraries like Pygame.\n",
    "    Overall, Python's simplicity, versatility, and strong community support make it a powerful tool for developers across various domains.\n",
    "    \"\"\",\n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "    Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data and improve their performance over time.\n",
    "    Key Concepts in Machine Learning:\n",
    "    1. Supervised Learning: In supervised learning, the model is trained on labeled data, meaning that each input has a corresponding output. Common algorithms include linear regression, decision trees, and support vector machines.\n",
    "    2. Unsupervised Learning: Unsupervised learning involves training the model on unlabeled data, allowing it to identify patterns and relationships. Clustering and dimensionality reduction are common techniques in this category.\n",
    "    3. Reinforcement Learning: In reinforcement learning, an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. This approach is often used in robotics and game playing.\n",
    "\n",
    "    Applications of Machine Learning:\n",
    "    Machine learning is widely used in various fields, including:\n",
    "    - Healthcare: For disease diagnosis and personalized treatment plans.\n",
    "    - Finance: For fraud detection and algorithmic trading.\n",
    "    - Marketing: For customer segmentation and recommendation systems.\n",
    "    - Autonomous Vehicles: For self-driving car technology.\n",
    "    \"\"\",\n",
    "}\n",
    "for file_path, content in samsple_texts.items():\n",
    "    with open(file_path, \"w\" ,encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample text files created.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have created two sample text files in ../data/text_files directory, one is python_intro.txt and another is machine_learning.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Python Programming Introduction\\n    Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and first released in 1991, Python has become one of the most popular programming languages in the world.\\n    Key Features of Python:\\n    1. Readability: Python's syntax is designed to be clear and easy to understand, making it an excellent choice for beginners and experienced developers alike.\\n    2. Versatility: Python can be used for a wide range of applications, including web development, data analysis, artificial intelligence, scientific computing, automation, and more.\\n    3. Extensive Libraries: Python boasts a rich ecosystem of libraries and frameworks, such as NumPy, pandas, Django, Flask, and TensorFlow, which facilitate rapid development and problem-solving.\\n    \")]\n"
     ]
    }
   ],
   "source": [
    "### Text Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\") # Reading the python_intro.txt file\n",
    "document =loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 547.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"Python Programming Introduction\\n    Python is a high-level, interpreted programming language known for its readability and versatility. Created by Guido van Rossum and first released in 1991, Python has become one of the most popular programming languages in the world.\\n    Key Features of Python:\\n    1. Readability: Python's syntax is designed to be clear and easy to understand, making it an excellent choice for beginners and experienced developers alike.\\n    2. Versatility: Python can be used for a wide range of applications, including web development, data analysis, artificial intelligence, scientific computing, automation, and more.\\n    3. Extensive Libraries: Python boasts a rich ecosystem of libraries and frameworks, such as NumPy, pandas, Django, Flask, and TensorFlow, which facilitate rapid development and problem-solving.\\n    \"),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n    Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead, these systems learn from data and improve their performance over time.\\n    Key Concepts in Machine Learning:\\n    1. Supervised Learning: In supervised learning, the model is trained on labeled data, meaning that each input has a corresponding output. Common algorithms include linear regression, decision trees, and support vector machines.\\n    2. Unsupervised Learning: Unsupervised learning involves training the model on unlabeled data, allowing it to identify patterns and relationships. Clustering and dimensionality reduction are common techniques in this category.\\n    3. Reinforcement Learning: In reinforcement learning, an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. This approach is often used in robotics and game playing.\\n\\n    Applications of Machine Learning:\\n    Machine learning is widely used in various fields, including:\\n    - Healthcare: For disease diagnosis and personalized treatment plans.\\n    - Finance: For fraud detection and algorithmic trading.\\n    - Marketing: For customer segmentation and recommendation systems.\\n    - Autonomous Vehicles: For self-driving car technology.\\n    ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## Load all text files from a directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"*.txt\",               # pattern to match text files\n",
    "    loader_cls=TextLoader,      # specify the loader class\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True,\n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00, 17.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 0}, page_content='Coordinate Attention for Efﬁcient Mobile Network Design\\nQibin Hou1\\nDaquan Zhou1\\nJiashi Feng1\\n1National University of Singapore\\n{andrewhoux,zhoudaquan21}@gmail.com\\nAbstract\\nRecent studies on mobile network design have demon-\\nstrated the remarkable effectiveness of channel atten-\\ntion (e.g., the Squeeze-and-Excitation attention) for lifting\\nmodel performance, but they generally neglect the posi-\\ntional information, which is important for generating spa-\\ntially selective attention maps. In this paper, we propose a\\nnovel attention mechanism for mobile networks by embed-\\nding positional information into channel attention, which\\nwe call “coordinate attention”. Unlike channel attention\\nthat transforms a feature tensor to a single feature vec-\\ntor via 2D global pooling, the coordinate attention factor-\\nizes channel attention into two 1D feature encoding pro-\\ncesses that aggregate features along the two spatial di-\\nrections, respectively. In this way, long-range dependen-\\ncies can be captured along one spatial direction and mean-\\nwhile precise positional information can be preserved along\\nthe other spatial direction. The resulting feature maps are\\nthen encoded separately into a pair of direction-aware and\\nposition-sensitive attention maps that can be complemen-\\ntarily applied to the input feature map to augment the rep-\\nresentations of the objects of interest. Our coordinate at-\\ntention is simple and can be ﬂexibly plugged into classic\\nmobile networks, such as MobileNetV2, MobileNeXt, and\\nEfﬁcientNet with nearly no computational overhead. Exten-\\nsive experiments demonstrate that our coordinate attention\\nis not only beneﬁcial to ImageNet classiﬁcation but more\\ninterestingly, behaves better in down-stream tasks, such as\\nobject detection and semantic segmentation. Code is avail-\\nable at https://github.com/Andrew-Qibin/\\nCoordAttention.\\n1. Introduction\\nAttention mechanisms, used to tell a model “what” and\\n“where” to attend, have been extensively studied [47, 29]\\nand widely deployed for boosting the performance of mod-\\nern deep neural networks [18, 44, 3, 25, 10, 14]. How-\\never, their application for mobile networks (with limited\\nmodel size) signiﬁcantly lags behind that for large networks\\nFigure 1. Performance of different attention methods on three clas-\\nsic vision tasks. The y-axis labels from left to right are top-1 ac-\\ncuracy, mean IoU, and AP, respectively. Clearly, our approach\\nnot only achieves the best result in ImageNet classiﬁcation [33]\\nagainst the SE block [18] and CBAM [44] but performs even better\\nin down-stream tasks, like semantic segmentation [9] and COCO\\nobject detection [21]. Results are based on MobileNetV2 [34].\\n[36, 13, 46]. This is mainly because the computational over-\\nhead brought by most attention mechanisms is not afford-\\nable for mobile networks.\\nConsidering the restricted computation capacity of mo-\\nbile networks, to date, the most popular attention mech-\\nanism for mobile networks is still the Squeeze-and-\\nExcitation (SE) attention [18]. It computes channel atten-\\ntion with the help of 2D global pooling and provides no-\\ntable performance gains at considerably low computational\\ncost. However, the SE attention only considers encoding\\ninter-channel information but neglects the importance of\\npositional information, which is critical to capturing object\\nstructures in vision tasks [42]. Later works, such as BAM\\n[30] and CBAM [44], attempt to exploit positional informa-\\ntion by reducing the channel dimension of the input tensor\\nand then computing spatial attention using convolutions as\\nshown in Figure 2(b). However, convolutions can only cap-\\nture local relations but fail in modeling long-range depen-\\ndencies that are essential for vision tasks [48, 14].\\nIn this paper, beyond the ﬁrst works, we propose a novel\\nand efﬁcient attention mechanism by embedding positional\\ninformation into channel attention to enable mobile net-\\nworks to attend over large regions while avoiding incur-\\nring signiﬁcant computation overhead. To alleviate the po-\\nsitional information loss caused by the 2D global pooling,\\nwe factorize channel attention into two parallel 1D feature\\nencoding processes to effectively integrate spatial coordi-\\narXiv:2103.02907v1  [cs.CV]  4 Mar 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 1}, page_content='nate information into the generated attention maps. Speciﬁ-\\ncally, our method exploits two 1D global pooling operations\\nto respectively aggregate the input features along the ver-\\ntical and horizontal directions into two separate direction-\\naware feature maps. These two feature maps with embed-\\nded direction-speciﬁc information are then separately en-\\ncoded into two attention maps, each of which captures long-\\nrange dependencies of the input feature map along one spa-\\ntial direction. The positional information can thus be pre-\\nserved in the generated attention maps. Both attention maps\\nare then applied to the input feature map via multiplication\\nto emphasize the representations of interest. We name the\\nproposed attention method as coordinate attention as its op-\\neration distinguishes spatial direction (i.e., coordinate) and\\ngenerates coordinate-aware attention maps.\\nOur coordinate attention offers the following advantages.\\nFirst of all, it captures not only cross-channel but also\\ndirection-aware and position-sensitive information, which\\nhelps models to more accurately locate and recognize the\\nobjects of interest. Secondly, our method is ﬂexible and\\nlight-weight, and can be easily plugged into classic build-\\ning blocks of mobile networks, such as the inverted resid-\\nual block proposed in MobileNetV2 [34] and the sandglass\\nblock proposed in MobileNeXt [49], to augment the fea-\\ntures by emphasizing informative representations. Thirdly,\\nas a pretrained model, our coordinate attention can bring\\nsigniﬁcant performance gains to down-stream tasks with\\nmobile networks, especially for those with dense predic-\\ntions (e.g., semantic segmentation), which we will show in\\nour experiment section.\\nTo demonstrate the advantages of the proposed approach\\nover previous attention methods for mobile networks, we\\nconduct extensive experiments in both ImageNet classiﬁ-\\ncation [33] and popular down-stream tasks, including ob-\\nject detection and semantic segmentation. With a compa-\\nrable amount of learnable parameters and computation, our\\nnetwork achieves 0.8% performance gain in top-1 classiﬁ-\\ncation accuracy on ImageNet. In object detection and se-\\nmantic segmentation, we also observe signiﬁcant improve-\\nments compared to models with other attention mechanisms\\nas shown in Figure 1. We hope our simple and efﬁcient\\ndesign could facilitate the development of attention mecha-\\nnisms for mobile networks in the future.\\n2. Related Work\\nIn this section, we give a brief literature review of this\\npaper, including prior works on efﬁcient network architec-\\nture design and attention or non-local models.\\n2.1. Mobile Network Architectures\\nRecent state-of-the-art mobile networks are mostly\\nbased on the depthwise separable convolutions [16] and\\nthe inverted residual block [34]. HBONet [20] introduces\\ndown-sampling operations inside each inverted residual\\nblock for modeling the representative spatial information.\\nShufﬂeNetV2 [27] uses a channel split module and a chan-\\nnel shufﬂe module before and after the inverted residual\\nblock. Later, MobileNetV3 [15] combines with neural ar-\\nchitecture search algorithms [50] to search for optimal ac-\\ntivation functions and the expansion ratio of inverted resid-\\nual blocks at different depths. Moreover, MixNet [39], Ef-\\nﬁcientNet [38] and ProxylessNAS [2] also adopt different\\nsearching strategies to search for either the optimal kernel\\nsizes of the depthwise separable convolutions or scalars to\\ncontrol the network weight in terms of expansion ratio, in-\\nput resolution, network depth and width. More recently,\\nZhou et al.\\n[49] rethought the way of exploiting depth-\\nwise separable convolutions and proposed MobileNeXt that\\nadopts a classic bottleneck structure for mobile networks.\\n2.2. Attention Mechanisms\\nAttention mechanisms [41, 40] have been proven helpful\\nin a variety of computer vision tasks, such as image classiﬁ-\\ncation [18, 17, 44, 1] and image segmentation [14, 19, 10].\\nOne of the successful examples is SENet [18], which sim-\\nply squeezes each 2D feature map to efﬁciently build inter-\\ndependencies among channels.\\nCBAM [44] further ad-\\nvances this idea by introducing spatial information encod-\\ning via convolutions with large-size kernels. Later works,\\nlike GENet [17], GALA [22], AA [1], and TA [28], extend\\nthis idea by adopting different spatial attention mechanisms\\nor designing advanced attention blocks.\\nNon-local/self-attention networks are recently very pop-\\nular due to their capability of building spatial or channel-\\nwise attention. Typical examples include NLNet [43], GC-\\nNet [3], A2Net [7], SCNet [25], GSoP-Net [11], or CC-\\nNet [19], all of which exploit non-local mechanisms to cap-\\nture different types of spatial information. However, be-\\ncause of the large amount of computation inside the self-\\nattention modules, they are often adopted in large mod-\\nels [13, 46] but not suitable for mobile networks.\\nDifferent from these approaches that leverage expensive\\nand heavy non-local or self-attention blocks, our approach\\nconsiders a more efﬁcient way of capturing positional in-\\nformation and channel-wise relationships to augment the\\nfeature representations for mobile networks. By factorizing\\nthe 2D global pooling operations into two one-dimensional\\nencoding processes, our approach performs much better\\nthan other attention methods with the lightweight property\\n(e.g., SENet [18], CBAM [44], and TA [28]).\\n3. Coordinate Attention\\nA coordinate attention block can be viewed as a com-\\nputational unit that aims to enhance the expressive power\\nof the learned features for mobile networks. It can take\\nany intermediate feature tensor X = [x1, x2, . . . , xC] ∈\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 2}, page_content='(a)\\nResidual\\nGlobal Avg Pool\\nFully Connected\\nNon-linear\\nFully Connected\\nSigmoid\\nRe-weight\\nInput\\nOutput\\nResidual\\nX Avg Pool\\nConcat + Conv2d\\nBatchNorm + Non-linear\\nConv2d\\nSigmoid\\nRe-weight\\nInput\\nOutput\\nC × H × W\\nY Avg Pool\\nConv2d\\nSigmoid\\nC × 1 × 1\\nC/r × 1 × 1\\nC/r × 1 × 1\\nC × 1 × 1\\nC × 1 × 1\\nC × H × W\\nC × H × W\\nC × 1 × W\\nC × H × 1\\nC/r × 1 × (W+H)\\nC/r × 1 × (W+H)\\nsplit\\nC × 1 × W\\nC × H × 1\\nC × H × 1\\nC × 1 × W\\nC × H × W\\n(c)\\n(b)\\nResidual\\nGAP + GMP\\nConv + ReLU\\n1×1 Conv\\nChannel Pool\\n7×7 Conv\\nRe-weight\\nInput\\nOutput\\nC × H × W\\nC × 1 × 1\\nC/r × 1 × 1\\nC × 1 × 1\\n2 × H × W\\n1 × H × W\\nC × H × W\\nRe-weight\\nSigmoid\\nBN + Sigmoid\\nC × H × W\\nFigure 2. Schematic comparison of the proposed coordinate attention block (c) to the classic SE channel attention block [18] (a) and CBAM\\n[44] (b). Here, “GAP” and “GMP” refer to the global average pooling and global max pooling, respectively. ‘X Avg Pool’ and ’Y Avg\\nPool’ refer to 1D horizontal global pooling and 1D vertical global pooling, respectively.\\nRC×H×W as input and outputs a transformed tensor with\\naugmented representations Y = [y1, y2, . . . , yC] of the\\nsame size to X. To provide a clear description of the pro-\\nposed coordinate attention, we ﬁrst revisit the SE attention,\\nwhich is widely used in mobile networks.\\n3.1. Revisit Squeeze-and-Excitation Attention\\nAs demonstrated in [18], the standard convolution it-\\nself is difﬁcult to model the channel relationships. Explic-\\nitly building channel inter-dependencies can increase the\\nmodel sensitivity to the informative channels that contribute\\nmore to the ﬁnal classiﬁcation decision. Moreover, using\\nglobal average pooling can also assist the model in captur-\\ning global information, which is a lack for convolutions.\\nStructurally, the SE block can be decomposed into two\\nsteps: squeeze and excitation, which are designed for global\\ninformation embedding and adaptive recalibration of chan-\\nnel relationships, respectively.\\nGiven the input X, the\\nsqueeze step for the c-th channel can be formulated as fol-\\nlows:\\nzc =\\n1\\nH × W\\nH\\nX\\ni=1\\nW\\nX\\nj=1\\nxc(i, j),\\n(1)\\nwhere zc is the output associated with the c-th channel. The\\ninput X is directly from a convolutional layer with a ﬁxed\\nkernel size and hence can be viewed as a collection of local\\ndescriptors. The squeeze operation makes collecting global\\ninformation possible.\\nThe second step, excitation, aims to fully capture\\nchannel-wise dependencies, which can be formulated as\\nˆX = X · σ(ˆz),\\n(2)\\nwhere · refers to channel-wise multiplication, σ is the sig-\\nmoid function, and ˆz is the result generated by a transfor-\\nmation function, which is formulated as follows:\\nˆz = T2(ReLU(T1(z))).\\n(3)\\nHere, T1 and T2 are two linear transformations that can be\\nlearned to capture the importance of each channel.\\nThe SE block has been widely used in recent mobile net-\\nworks [18, 4, 38] and proven to be a key component for\\nachieving state-of-the-art performance. However, it only\\nconsiders reweighing the importance of each channel by\\nmodeling channel relationships but neglects positional in-\\nformation, which as we will prove experimentally in Sec-\\ntion 4 to be important for generating spatially selective at-\\ntention maps. In the following, we introduce a novel at-\\ntention block, which takes into account both inter-channel\\nrelationships and positional information.\\n3.2. Coordinate Attention Blocks\\nOur coordinate attention encodes both channel relation-\\nships and long-range dependencies with precise positional\\ninformation in two steps: coordinate information embed-\\nding and coordinate attention generation. The diagram of\\nthe proposed coordinate attention block can be found in the\\nright part of Figure 2. In the following, we will describe it\\nin detail.\\n3.2.1\\nCoordinate Information Embedding\\nThe global pooling is often used in channel attention to\\nencode spatial information globally, but it squeezes global\\nspatial information into a channel descriptor and hence is\\ndifﬁcult to preserve positional information, which is essen-\\ntial for capturing spatial structures in vision tasks. To en-\\ncourage attention blocks to capture long-range interactions\\nspatially with precise positional information, we factorize\\nthe global pooling as formulated in Eqn. (1) into a pair of\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 3}, page_content='1D feature encoding operations. Speciﬁcally, given the in-\\nput X, we use two spatial extents of pooling kernels (H, 1)\\nor (1, W) to encode each channel along the horizontal co-\\nordinate and the vertical coordinate, respectively. Thus, the\\noutput of the c-th channel at height h can be formulated as\\nzh\\nc (h) = 1\\nW\\nX\\n0≤i<W\\nxc(h, i).\\n(4)\\nSimilarly, the output of the c-th channel at width w can be\\nwritten as\\nzw\\nc (w) = 1\\nH\\nX\\n0≤j<H\\nxc(j, w).\\n(5)\\nThe above two transformations aggregate features along\\nthe two spatial directions respectively, yielding a pair of\\ndirection-aware feature maps. This is rather different from\\nthe squeeze operation (Eqn. (1)) in channel attention meth-\\nods that produce a single feature vector. These two transfor-\\nmations also allow our attention block to capture long-range\\ndependencies along one spatial direction and preserve pre-\\ncise positional information along the other spatial direction,\\nwhich helps the networks more accurately locate the objects\\nof interest.\\n3.2.2\\nCoordinate Attention Generation\\nAs described above, Eqn. (4) and Eqn. (5) enable a global\\nreceptive ﬁeld and encode precise positional information.\\nTo take advantage of the resulting expressive representa-\\ntions, we present the second transformation, termed coordi-\\nnate attention generation. Our design refers to the following\\nthree criteria. First of all, the new transformation should be\\nas simple and cheap as possible regarding the applications\\nin mobile environments. Second, it can make full use of the\\ncaptured positional information so that the regions of inter-\\nest can be accurately highlighted. Last but not the least, it\\nshould also be able to effectively capture inter-channel rela-\\ntionships, which has been demonstrated essential in existing\\nstudies [18, 44].\\nSpeciﬁcally, given the aggregated feature maps produced\\nby Eqn. 4 and Eqn. 5, we ﬁrst concatenate them and then\\nsend them to a shared 1 × 1 convolutional transformation\\nfunction F1, yielding\\nf = δ(F1([zh, zw])),\\n(6)\\nwhere [·, ·] denotes the concatenation operation along the\\nspatial dimension, δ is a non-linear activation function and\\nf ∈RC/r×(H+W ) is the intermediate feature map that en-\\ncodes spatial information in both the horizontal direction\\nand the vertical direction. Here, r is the reduction ratio\\nfor controlling the block size as in the SE block. We then\\nsplit f along the spatial dimension into two separate tensors\\n(a)\\nConv 1×1\\nConv 1×1\\n+\\nDwise 3×3\\nAttention\\n(b)\\nDwise 3×3\\nConv 1×1\\nConv 1×1\\nDwise 3×3\\n+\\n×\\nAttention\\n×\\nFigure 3. Network implementation for different network architec-\\ntures. (a) Inverted residual block proposed in MobileNetV2 [34];\\n(b) Sandglass bottleneck block proposed in MobileNeXt [49].\\nf h ∈RC/r×H and f w ∈RC/r×W . Another two 1 × 1 con-\\nvolutional transformations Fh and Fw are utilized to sepa-\\nrately transform f h and f w to tensors with the same channel\\nnumber to the input X, yielding\\ngh = σ(Fh(f h)),\\n(7)\\ngw = σ(Fw(f w)).\\n(8)\\nRecall that σ is the sigmoid function. To reduce the over-\\nhead model complexity, we often reduce the channel num-\\nber of f with an appropriate reduction ratio r (e.g., 32). We\\nwill discuss the impact of different reduction ratios on the\\nperformance in our experiment section. The outputs gh and\\ngw are then expanded and used as attention weights, respec-\\ntively. Finally, the output of our coordinate attention block\\nY can be written as\\nyc(i, j) = xc(i, j) × gh\\nc (i) × gw\\nc (j).\\n(9)\\nDiscussion. Unlike channel attention that only focuses on\\nreweighing the importance of different channels, our coor-\\ndinate attention block also considers encoding the spatial\\ninformation. As described above, the attention along both\\nthe horizontal and vertical directions is simultaneously ap-\\nplied to the input tensor. Each element in the two attention\\nmaps reﬂects whether the object of interest exists in the cor-\\nresponding row and column. This encoding process allows\\nour coordinate attention to more accurately locate the exact\\nposition of the object of interest and hence helps the whole\\nmodel to recognize better. We will demonstrate this exhaus-\\ntively in our experiment section.\\n3.3. Implementation\\nAs the goal of this paper is to investigate a better way\\nto augment the convolutional features for mobile networks,\\nhere we take two classic light-weight architectures with dif-\\nferent types of residual blocks (i.e., MobileNetV2 [34] and\\nMobileNeXt [49]) as examples to demonstrate the advan-\\ntages of the proposed coordinate attention block over other\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 4}, page_content='Table 1. Result comparisons under different experiment settings\\nof the proposed coordinate attention. Here, r is the reduction ratio\\nand the baseline result is based on the MobileNetV2 model. As\\ncan be seen, the model with either the horizontal (X) attention or\\nthe vertical (Y) attention added achieves the same performance as\\nthe one with SE attention. However, when taking both horizon-\\ntal and vertical attentions into account (coordinate attention), our\\napproach yields the best result. The latency is tested on a Google\\nPixel 4 device.\\nSettings\\nParam. M-Adds\\nr\\nLatency Top-1 (%)\\nBaseline\\n3.5M\\n300M\\n-\\n14-16ms\\n72.3\\n+ SE\\n3.89M\\n300M\\n24 16-18ms\\n73.5+1.2\\n+ X Attention\\n3.89M\\n300M\\n24 16-18ms\\n73.5+1.2\\n+ Y Attention\\n3.89M\\n300M\\n24 16-18ms\\n73.5+1.2\\n+ Coord. Attention 3.95M\\n310M\\n32 17-19ms 74.3+2.0\\nfamous light-weight attention blocks. Figure 3 shows how\\nwe plug attention blocks into the inverted residual block in\\nMobileNetV2 and the sandglass block in MobileNeXt.\\n4. Experiments\\nIn this section, we ﬁrst describe our experiment settings\\nand then conduct a series of ablation experiments to demon-\\nstrate the contribution of each component in the proposed\\ncoordinate attention to the performance. Next, we compare\\nour approach with some attention based methods. Finally,\\nwe report the results of the proposed approach compared to\\nother attention based methods on object detection and se-\\nmantic segmentation.\\n4.1. Experiment Setup\\nWe use the PyTorch toolbox [31] to implement all our\\nexperiments. During training, we use the standard SGD op-\\ntimizer with decay and momentum of 0.9 to train all the\\nmodels. The weight decay is set to 4 × 10−5 always. The\\ncosine learning schedule with an initial learning rate of 0.05\\nis adopted. We use four NVIDIA GPUs for training and the\\nbatch size is set to 256. Without extra declaration, we take\\nMobileNetV2 as our baseline and train all the models for\\n200 epochs. For data augmentation, we use the same meth-\\nods as in MobileNetV2. We report results on the ImageNet\\ndataset [33] in classiﬁcation.\\n4.2. Ablation Studies\\nImportance of coordinate attention. To demonstrate the\\nperformance of the proposed coordinate attention, we per-\\nform a series of ablation experiments, the corresponding re-\\nsults of which are all listed in Table 1. We remove either the\\nhorizontal attention or the vertical attention from the coordi-\\nnate attention to see the importance of encoding coordinate\\ninformation. As shown in Table 1, the model with atten-\\ntion along either direction has comparable performance to\\nTable 2. Comparisons of different attention methods under differ-\\nent weight multipliers when taking MobileNetV2 as the baseline.\\nSettings\\nParam. (M) M-Adds (M) Top-1 Acc (%)\\nMobileNetV2-1.0\\n3.5\\n300\\n72.3\\n+ SE\\n3.89\\n300\\n73.5+1.2\\n+ CBAM\\n3.89\\n300\\n73.6+1.3\\n+ CA\\n3.95\\n310\\n74.3+2.0\\nMobileNetV2-0.75\\n2.5\\n200\\n69.9\\n+ SE\\n2.86\\n210\\n71.5+1.6\\n+ CBAM\\n2.86\\n210\\n71.5+1.6\\n+ CA\\n2.89\\n210\\n72.1+2.2\\nMobileNetV2-0.5\\n2.0\\n100\\n65.4\\n+ SE\\n2.1\\n100\\n66.4+1.0\\n+ CBAM\\n2.1\\n100\\n66.4+1.0\\n+ CA\\n2.1\\n100\\n67.0+1.6\\nthe one with the SE attention. However, when both the hor-\\nizontal attention and the vertical attention are incorporated,\\nwe obtain the best result as highlighted in Table 1. These ex-\\nperiments reﬂect that with comparable learnable parameters\\nand computational cost, coordinate information embedding\\nis more helpful for image classiﬁcation.\\nDifferent weight multipliers.\\nHere, we take two clas-\\nsic mobile networks (including MobileNetV2 [34] with in-\\nverted residual blocks and MobileNeXt [49] with sandglass\\nbottleneck block) as baselines to see the performance of\\nthe proposed approach compared to the SE attention [18]\\nand CBAM [44] under different weight multipliers. In this\\nexperiment, we adopt three typical weight multipliers, in-\\ncluding {1.0, 0.75, 0.5}. As shown in Table 2, when taking\\nthe MobileNetV2 network as baseline, models with CBAM\\nhave similar results to those with the SE attention. However,\\nmodels with the proposed coordinate attention yield the best\\nresults under each setting. Similar phenomenon can also be\\nobserved when the MobileNeXt network is used as listed in\\nTable 3. This indicates that no matter which of the sandglass\\nbottleneck block or the inverted residual block is considered\\nand no matter which weight multiplier is selected, our coor-\\ndinate attention performs the best because of the advanced\\nway to encode positional and inter-channel information si-\\nmultaneously.\\nThe impact of reduction ratio r. To investigate the im-\\npact of different reduction ratios of attention blocks on the\\nmodel performance, we attempt to decrease the size of the\\nreduction ratio and see the performance change. As shown\\nin Table 4, when we reduce r to half of the original size, the\\nmodel size increases but better performance can be yielded.\\nThis demonstrates that adding more parameters by reducing\\nthe reduction ratio matters for improving the model perfor-\\nmance. More importantly, our coordinate attention still per-\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 5}, page_content='Table 3. Comparisons of different attention methods under differ-\\nent weight multipliers when taking MobileNeXt [49] as the base-\\nline.\\nSettings\\nParam. (M) M-Adds (M) Top-1 Acc (%)\\nMobileNeXt\\n3.5\\n300\\n74.0\\n+ SE\\n3.89\\n300\\n74.7+0.7\\n+ CA\\n4.09\\n330\\n75.2+1.2\\nMobileNeXt-0.75\\n2.5\\n210\\n72.0\\n+ SE\\n2.9\\n210\\n72.6+0.6\\n+ CA\\n3.0\\n220\\n73.2+1.2\\nMobileNeXt-0.5\\n2.1\\n110\\n67.7\\n+ SE\\n2.4\\n110\\n68.7+1.0\\n+ CA\\n2.4\\n110\\n69.4+1.7\\nTable 4. Comparisons of models equipped with different attention\\nblocks under different reduction ratios r. The baseline result is\\nbased on the MobileNetV2 model. Obviously, when the reduction\\nratio decreases, our approach still yields the best results.\\nSettings\\nParam.\\nM-Adds\\nr\\nTop-1 Acc (%)\\nBaseline\\n3.5M\\n300M\\n-\\n72.3\\n+ SE\\n3.89M\\n300M\\n24\\n73.5+1.2\\n+ CBAM\\n3.89M\\n300M\\n24\\n73.6+1.3\\n+ CA (Ours)\\n3.95M\\n310M\\n32\\n74.3+2.0\\n+ SE\\n4.28M\\n300M\\n12\\n74.1+1.8\\n+ CBAM\\n4.28M\\n300M\\n12\\n74.1+1.8\\n+ CA (Ours)\\n4.37M\\n310M\\n16\\n74.7+2.4\\nforms better than the SE attention and CBAM in this exper-\\niment, reﬂecting the robustness of the proposed coordinate\\nattention to the reduction ratio.\\n4.3. Comparison with Other Methods\\nAttention for Mobile Networks. We compare our coor-\\ndinate attention with other light-weight attention methods\\nfor mobile networks, including the widely adopted SE at-\\ntention [18] and CBAM [44] in Table 2. As can be seen,\\nadding the SE attention has already raised the classiﬁcation\\nperformance by more than 1%. For CBAM, it seems that its\\nspatial attention module shown in Figure 2(b) does not con-\\ntribute in mobile networks compared to the SE attention.\\nHowever, when the proposed coordinate attention is con-\\nsidered, we achieve the best results. We also visualize the\\nfeature maps produced by models with different attention\\nmethods in Figure 4. Obviously, our coordinate attention\\ncan help better in locating the objects of interest than the\\nSE attention and CBAM.\\nWe argue that the advantages of the proposed positional\\ninformation encoding manner over CBAM are two-fold.\\nFirst, the spatial attention module in CBAM squeezes the\\nBef. SE\\nAft. SE\\nBef. CBAM\\nAft. CBAM\\nBef. CA\\nAft. CA\\nLeaf beetle\\nFlamingo\\nScreen\\nBeer glass\\nBlack bear\\nFigure 4. Visualization of feature maps produced by models with\\ndifferent attention methods in the last building block.\\nWe use\\nGrad-CAM [35] as our visualization tool. Both feature maps be-\\nfore and after each attention block are visualized. It is obvious\\nthat our coordinate attention (CA) can more precisely locate the\\nobjects of interest than other attention methods.\\nchannel dimension to 1, leading to information loss. How-\\never, our coordinate attention uses an appropriate reduction\\nratio to reduce the channel dimension in the bottleneck,\\navoiding too much information loss. Second, CBAM uti-\\nlizes a convolutional layer with kernel size 7 × 7 to encode\\nlocal spatial information while our coordinate attention en-\\ncodes global information by using two complementary 1D\\nglobal pooling operations. This enables our coordinate at-\\ntention to capture long-range dependencies among spatial\\nlocations that are essential for vision tasks.\\nStronger Baseline. To further demonstrate the advantages\\nof the proposed coordinate attention over the SE attention\\nin more powerful mobile networks, we take EfﬁcientNet-b0\\n[38] as our baseline here. EfﬁcientNet is based on archi-\\ntecture search algorithms. and contains SE attention. To\\ninvestigate the performance of the proposed coordinate at-\\ntention on EfﬁcientNet, we simply replace the SE attention\\nwith our proposed coordinate attention. For other settings,\\nwe follow the original paper. The results have been listed in\\nTable 5. Compared to the original EfﬁcientNet-b0 with SE\\nattention included and other methods that have comparable\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 6}, page_content='Table\\n5.\\nExperimental\\nresults\\nwhen\\ntaking\\nthe\\npowerful\\nEfﬁcientNet-b0 [38] as baseline.\\nWe also compare with other\\nmethods that have comparable parameters and computations to\\nEfﬁcientNet-b0.\\nSettings\\nParam. M-Adds Top-1 Acc (%)\\nPNAS [23]\\n5.1M\\n588M\\n72.7\\nDARTS [24]\\n4.7M\\n574M\\n73.3\\nProxylessNAS-M [2]\\n4.1M\\n330M\\n74.4\\nAmoebaNet-A [32]\\n5.1M\\n555M\\n74.5\\nFBNet-C [45]\\n5.5M\\n375M\\n74.9\\nMobileNeXt [49]\\n6.1M\\n590M\\n76.1\\nMNasNet-A3 [37]\\n5.2M\\n403M\\n76.7\\nEfﬁcientNet-b0 (w/ SE) [38]\\n5.3M\\n390M\\n76.3\\nEfﬁcientNet-b0 (w/ CA)\\n5.4M\\n400M\\n76.9\\nparameters and computations to EfﬁcientNet-b0, our net-\\nwork with coordinate attention achieves the best result. This\\ndemonstrates that the proposed coordinate attention can still\\nperformance well in powerful mobile networks.\\n4.4. Applications\\nIn this subsection, we conduct experiments on both the\\nobject detection task and the semantic segmentation task to\\nexplore the transferable capability of the proposed coordi-\\nnate attention against other attention methods.\\n4.4.1\\nObject Detection\\nImplementation Details. Our code is based on PyTorch\\nand SSDLite [34, 26]. Following [34], we connect the ﬁrst\\nand second layers of SSDLite to the last pointwise convo-\\nlutions with output stride of 16 and 32, respectively and\\nadd the rest SSDLite layers on top of the last convolutional\\nlayer. When training on COCO, we set the batch size to\\n256 and use the synchronized batch normalization. The co-\\nsine learning schedule is used with an initial learning rate of\\n0.01. We train the models for totally 1,600,000 iterations.\\nWhen training on Pascal VOC, the batch size is set to 24\\nand all the models are trained for 240,000 iterations. The\\nweight decay is set to 0.9. The initial learning rate is 0.001,\\nwhich is then divided by 10 at 160,000 and again at 200,000\\niterations. For other settings, readers can refer to [34, 26].\\nResults on COCO. In this experiment, we follow most pre-\\nvious work and report results in terms of AP, AP50, AP75,\\nAPS, APM, and APL, respectively. In Table 6, we show the\\nresults produced by different network settings on the COCO\\n2017 validation set. It is obvious that adding coordinate at-\\ntention into MobileNetV2 substantially improve the detec-\\ntion results (24.5 v.s. 22.3) with only 0.5M parameters over-\\nhead and nearly the same computational cost. Compared to\\nother light-weight attention methods, such as the SE atten-\\ntion and CBAM, our version of SSDLite320 achieves the\\nbest results in all metrics with nearly the same number of\\nparameters and computations.\\nMoreover, we also show results produced by previous\\nstate-of-the-art models based on SSDLite320 as listed in Ta-\\nble 6. Note that some methods (e.g., MobileNetV3 [15] and\\nMnasNet-A1 [37]) are based on neural architecture search\\nmethods but our model does not. Obviously, our detection\\nmodel achieves the best results in terms of AP compared to\\nother approaches with close parameters and computations.\\nResults on Pascal VOC. In Table 7, we show the detection\\nresults on Pascal VOC 2007 test set when different atten-\\ntion methods are adopted. We observe that the SE attention\\nand CBAM cannot improve the baseline results. However,\\nadding the proposed coordinate attention can largely raise\\nthe mean AP from 71.7 to 73.1. Both detection experiments\\non COCO and Pascal VOC datasets demonstrate that classi-\\nﬁcation models with the proposed coordinate attention have\\nbetter transferable capability compared to those with other\\nattention methods.\\n4.4.2\\nSemantic Segmentation\\nWe also conduct experiments on semantic segmenta-\\ntion. Following MobileNetV2 [34], we utilize the classic\\nDeepLabV3 [6] as an example and compare the proposed\\napproach with other models to demonstrate the transferable\\ncapability of the proposed coordinate attention in semantic\\nsegmentation. Speciﬁcally, we discard the last linear opera-\\ntor and connect the ASPP to the last convolutional operator.\\nWe replace the standard 3 × 3 convolutional operators with\\nthe depthwise separable convolutions in the ASPP to reduce\\nthe model size considering mobile applications. The output\\nchannels for each branch in ASPP are set to 256 and other\\ncomponents in the ASPP are kept unchanged (including the\\n1×1 convolution branch and the image-level feature encod-\\ning branch). We report results on two widely used semantic\\nsegmentation benchmarks, including Pascal VOC 2012 [9]\\nand Cityscapes [8]. For experiment settings, we strictly fol-\\nlow the DeeplabV3 paper except for the weight decay that is\\nset to 4e-5. When the output stride is set to 16, the dilation\\nrates in the ASPP are {6, 12, 18} while {12, 24, 36} when\\nthe output stride is set to 8.\\nResults on Pascal VOC 2012. The Pascal VOC 2012 seg-\\nmentation benchmark has totally 21 classes including one\\nbackground class. As suggested by the original paper, we\\nuse the split with 1,464 images for training and the split\\nwith 1,449 images for validation. Also, as done in most\\nprevious work [6, 5], we augment the training set by adding\\nextra images from [12], resulting in totally 10,582 images\\nfor training.\\nWe show the segmentation results when taking differ-\\nent models as backbones in Table 8. We report results un-\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 7}, page_content='Table 6. Object detection results on the COCO validation set. In all experiments here, we use the SSDLite320 detector. As can be seen, the\\nbackbone model with our coordinate attention achieves the best results in terms of all kinds of measuring metrics. Note that all the results\\nare based on single-model test. Besides hand-designed mobile networks, we also show results produced by architecture search-based\\nmethods (i.e., MobileNetV3 [15] and MnasNet-A1 [37]).\\nNo.\\nMethod\\nBackbone\\nParam. (M)\\nM-Adds (B)\\nAP\\nAP50\\nAP75\\nAPS\\nAPM\\nAPL\\n1\\nSSDLite320\\nMobileNetV1 [16]\\n5.1\\n1.3\\n22.2\\n-\\n-\\n-\\n-\\n-\\n2\\nSSDLite320\\nMobileNetV2 [34]\\n4.3\\n0.8\\n22.3\\n37.4\\n22.7\\n2.8\\n21.2\\n42.8\\n3\\nSSDLite320\\nMobileNetV3 [15]\\n5.0\\n0.62\\n22.0\\n-\\n-\\n-\\n-\\n-\\n4\\nSSDLite320\\nMnasNet-A1 [37]\\n4.9\\n0.8\\n23.0\\n-\\n-\\n3.8\\n21.7\\n42.0\\n5\\nSSDLite320\\nMobileNeXt [49]\\n4.4\\n0.8\\n23.3\\n38.9\\n23.7\\n2.8\\n22.7\\n45.0\\n6\\nSSDLite320\\nMobileNetV2 + SE\\n4.7\\n0.8\\n23.7\\n40.0\\n24.3\\n2.2\\n25.4\\n44.7\\n7\\nSSDLite320\\nMobileNetV2 + CBAM\\n4.7\\n0.8\\n23.0\\n38.6\\n23.3\\n2.7\\n22.2\\n44.5\\n8\\nSSDLite320\\nMobileNetV2 + CA\\n4.8\\n0.8\\n24.5\\n40.7\\n25.4\\n2.3\\n26.2\\n45.9\\nTable 7. Object detection results on the Pascal VOC 2007 test\\nset. We can observe that when the same SSDLite320 detector\\nis adopted, MobileNetV2 network with our coordinate attention\\nadded achieves better results in terms of mAP.\\nBackbone\\nParam. (M) M-Adds (B) mAP (%)\\nMobileNetV2 [34]\\n4.3\\n0.8\\n71.7\\nMobileNetV2 + SE\\n4.7\\n0.8\\n71.7\\nMobileNetV2 + CBAM\\n4.7\\n0.8\\n71.7\\nMobileNetV2 + CA\\n4.8\\n0.8\\n73.1\\nTable 8. Semantic segmentation results on the Pascal VOC 2012\\nvalidation set. All the results are based on single-model test and\\nno post-processing tools are used. We can see that the models\\nequipped with all attention methods improve the segmentation re-\\nsults. However, when the proposed coordinate attention is used,\\nwe achieve the best result, which is much better than models with\\nother attention methods. ‘Stride’ here denotes the output stride of\\nthe segmentation network.\\nBackbone\\nParam. (M)\\nStride\\nmIoU (%)\\nMobileNetV2 [34]\\n4.5\\n16\\n70.84\\nMobileNetV2 + SE\\n4.9\\n16\\n71.69\\nMobileNetV2 + CBAM\\n4.9\\n16\\n71.28\\nMobileNetV2 + CA (ours)\\n5.0\\n16\\n73.32\\nMobileNetV2 [34]\\n4.5\\n8\\n71.82\\nMobileNetV2 + SE\\n4.9\\n8\\n72.52\\nMobileNetV2 + CBAM\\n4.9\\n8\\n71.67\\nMobileNetV2 + CA (ours)\\n5.0\\n8\\n73.96\\nder two different output strides, i.e., 16 and 8. Note that\\nall the results reported here are not based on COCO pre-\\ntraining. According to Table 8, models equipped with our\\ncoordinate attention performs much better than the vanilla\\nMobileNetV2 and other attention methods.\\nResults on Cityscapes. Cityscapes [8] is one of the most\\nTable 9. Semantic segmentation results on the Cityscapes [8] val-\\nidation set. We report results on single-model test and full image\\nsize (i.e., 1024 × 2048) is used for testing. We do not use any\\npost-processing tools.\\nBackbone\\nParam. (M) Output Stride mIoU (%)\\nMobileNetV2\\n4.5\\n8\\n71.4\\nMobileNetV2 + SE\\n4.9\\n8\\n72.2\\nMobileNetV2 + CBAM\\n4.9\\n8\\n71.4\\nMobileNetV2 + CA\\n5.0\\n8\\n74.0\\npopular urban street scene segmentation datasets, contain-\\ning totally 19 different categories. Following the ofﬁcial\\nsuggestion, we use the split with 2,975 images for training\\nand 500 images for validation. Only the ﬁne-annotated im-\\nages are used for training. In training, we randomly crop\\nthe original images to 768×768. During testing, all images\\nare kept the original size (1024 × 2048).\\nIn Table 9, we show the segmentation results pro-\\nduced by models with different attention methods on the\\nCityscapes dataset. Compared to the vanilla MobileNetV2\\nand other attention methods, our coordinate attention can\\nimprove the segmentation results by a large margin with\\ncomparable number of learnable parameters.\\nDiscussion. We observe that our coordinate attention yields\\nlarger improvement on semantic segmentation than Ima-\\ngeNet classiﬁcation and object detection.\\nWe argue that\\nthis is because our coordinate attention is able to capture\\nlong-range dependencies with precise postional informa-\\ntion, which is more beneﬁcial to vision tasks with dense\\npredictions, such as semantic segmentation.\\n5. Conclusions\\nIn this paper, we present a novel light-weight atten-\\ntion mechanism for mobile networks, named coordinate\\nattention.\\nOur coordinate attention inherits the advan-\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 8}, page_content='tage of channel attention methods (e.g., the Squeeze-and-\\nExcitation attention) that model inter-channel relationships\\nand meanwhile captures long-range dependencies with pre-\\ncise positional information.\\nExperiments in ImageNet\\nclassiﬁcation, object detection and semantic segmentation\\ndemonstrate the effectiveness of our coordination attention.\\nAcknowledgement.\\nThis research was partially sup-\\nported by AISG-100E-2019-035,\\nMOE2017-T2-2-151,\\nNUS ECRA FY17 P08 and CRP20-2017-0006.\\nReferences\\n[1] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V Le.\\nAttention augmented convolutional net-\\nworks. arXiv preprint arXiv:1904.09925, 2019.\\n[2] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\\nneural architecture search on target task and hardware. arXiv\\npreprint arXiv:1812.00332, 2018.\\n[3] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\\nHu.\\nGcnet: Non-local networks meet squeeze-excitation\\nnetworks and beyond.\\nIn Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision Workshops, pages\\n0–0, 2019.\\n[4] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George\\nPapandreou, Barret Zoph, Florian Schroff, Hartwig Adam,\\nand Jon Shlens. Searching for efﬁcient multi-scale architec-\\ntures for dense image prediction. In NeurIPS, pages 8699–\\n8710, 2018.\\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\\nKevin Murphy, and Alan L Yuille. Semantic image segmen-\\ntation with deep convolutional nets and fully connected crfs.\\nIn ICLR, 2015.\\n[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017.\\n[7] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. Aˆ 2-nets: Double attention networks.\\nIn Advances in neural information processing systems, pages\\n352–361, 2018.\\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld,\\nMarkus Enzweiler,\\nRodrigo Benenson,\\nUwe\\nFranke, Stefan Roth, and Bernt Schiele.\\nThe cityscapes\\ndataset for semantic urban scene understanding. In CVPR,\\n2016.\\n[9] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\\npher KI Williams, John Winn, and Andrew Zisserman. The\\npascal visual object classes challenge: A retrospective. IJCV,\\n2015.\\n[10] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\\nFang, and Hanqing Lu.\\nDual attention network for scene\\nsegmentation. In CVPR, pages 3146–3154, 2019.\\n[11] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks. In Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 3024–3033, 2019.\\n[12] Bharath Hariharan, Pablo Arbel´aez, Lubomir Bourdev,\\nSubhransu Maji, and Jitendra Malik. Semantic contours from\\ninverse detectors. In ICCV, 2011.\\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016.\\n[14] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng.\\nStrip pooling: Rethinking spatial pooling for scene parsing.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 4003–4012, 2020.\\n[15] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\\nbilenetv3. In ICCV, pages 1314–1324, 2019.\\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\\ntional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861, 2017.\\n[17] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in convo-\\nlutional neural networks. In Advances in neural information\\nprocessing systems, pages 9401–9411, 2018.\\n[18] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\\nworks. In CVPR, pages 7132–7141, 2018.\\n[19] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\\nHuang, Yunchao Wei, and Wenyu Liu.\\nCcnet:\\nCriss-\\ncross attention for semantic segmentation.\\narXiv preprint\\narXiv:1811.11721, 2018.\\n[20] Duo Li, Aojun Zhou, and Anbang Yao. Hbonet: Harmonious\\nbottleneck on two orthogonal dimensions. In Proceedings\\nof the IEEE International Conference on Computer Vision,\\npages 3316–3325, 2019.\\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nECCV, 2014.\\n[22] Drew Linsley, Dan Shiebler, Sven Eberhardt, and Thomas\\nSerre. Learning what and where to attend. In ICLR, 2019.\\n[23] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan\\nHuang, and Kevin Murphy. Progressive neural architecture\\nsearch. In Proceedings of the European Conference on Com-\\nputer Vision (ECCV), pages 19–34, 2018.\\n[24] Hanxiao\\nLiu,\\nKaren\\nSimonyan,\\nand\\nYiming\\nYang.\\nDarts: Differentiable architecture search.\\narXiv preprint\\narXiv:1806.09055, 2018.\\n[25] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Changhu\\nWang, and Jiashi Feng. Improving convolutional networks\\nwith self-calibrated convolutions.\\nIn Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10096–10105, 2020.\\n[26] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\\nBerg. Ssd: Single shot multibox detector. In ECCV, 2016.\\n[27] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-05T02:07:26+00:00', 'source': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'file_path': '../data/pdf/Effieient_Mobile_Network_Design.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-05T02:07:26+00:00', 'trapped': '', 'modDate': 'D:20210305020726Z', 'creationDate': 'D:20210305020726Z', 'page': 9}, page_content='ture design. In Proceedings of the European conference on\\ncomputer vision (ECCV), pages 116–131, 2018.\\n[28] Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai,\\nand Qibin Hou. Rotate to attend: Convolutional triplet atten-\\ntion module. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision, pages 3139–\\n3148, 2021.\\n[29] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Re-\\ncurrent models of visual attention. In Advances in neural\\ninformation processing systems, pages 2204–2212, 2014.\\n[30] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\\nKweon. Bam: Bottleneck attention module. arXiv preprint\\narXiv:1807.06514, 2018.\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\\nimperative style, high-performance deep learning library. In\\nNeurIPS, pages 8024–8035, 2019.\\n[32] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\\nLe. Regularized evolution for image classiﬁer architecture\\nsearch. In Proceedings of the aaai conference on artiﬁcial\\nintelligence, volume 33, pages 4780–4789, 2019.\\n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al.\\nImagenet large\\nscale visual recognition challenge. IJCV, 115(3):211–252,\\n2015.\\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\\nmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted\\nresiduals and linear bottlenecks. In CVPR, pages 4510–4520,\\n2018.\\n[35] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\\nGrad-cam:\\nVisual explanations from deep networks via\\ngradient-based localization. In ICCV, pages 618–626, 2017.\\n[36] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. In ICLR,\\n2015.\\n[37] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le.\\nMnas-\\nnet: Platform-aware neural architecture search for mobile.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 2820–2828, 2019.\\n[38] Mingxing Tan and Quoc V Le.\\nEfﬁcientnet: Rethinking\\nmodel scaling for convolutional neural networks. In ICML,\\n2019.\\n[39] Mingxing Tan and Quoc V Le. Mixconv: Mixed depthwise\\nconvolutional kernels. CoRR, abs/1907.09595, 2019.\\n[40] John K Tsotsos. A computational perspective on visual at-\\ntention. MIT Press, 2011.\\n[41] John K Tsotsos et al. Analyzing vision at the complexity\\nlevel. Behavioral and brain sciences, 13(3):423–469, 1990.\\n[42] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\\nalone axial-attention for panoptic segmentation.\\narXiv\\npreprint arXiv:2003.07853, 2020.\\n[43] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local neural networks. In CVPR, 2018.\\n[44] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In\\nSo Kweon. Cbam: Convolutional block attention module.\\nIn Proceedings of the European conference on computer vi-\\nsion (ECCV), pages 3–19, 2018.\\n[45] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-\\nvnet design via differentiable neural architecture search. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 10734–10742, 2019.\\n[46] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In CVPR, 2017.\\n[47] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\\nBengio. Show, attend and tell: Neural image caption gen-\\neration with visual attention. In ICML, pages 2048–2057,\\n2015.\\n[48] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\\nWang, and Jiaya Jia. Pyramid scene parsing network. In\\nCVPR, 2017.\\n[49] Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and\\nShuicheng Yan. Rethinking bottleneck structure for efﬁcient\\nmobile network design. In ECCV, 2020.\\n[50] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\\nLe. Learning transferable architectures for scalable image\\nrecognition. In CVPR, pages 8697–8710, 2018.\\n10'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 0}, page_content='Anchor DETR: Query Design for Transformer-Based Object Detection\\nYingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun\\nMEGVII Technology\\n{wangyingming, zhangxiangyu, yangtong, sunjian}@megvii.com\\nAbstract\\nIn this paper, we propose a novel query design for the\\ntransformer-based object detection. In previous transformer-\\nbased detectors, the object queries are a set of learned em-\\nbeddings. However, each learned embedding does not have\\nan explicit physical meaning and we cannot explain where\\nit will focus on. It is difﬁcult to optimize as the prediction\\nslot of each object query does not have a speciﬁc mode. In\\nother words, each object query will not focus on a speciﬁc\\nregion. To solve these problems, in our query design, object\\nqueries are based on anchor points, which are widely used\\nin CNN-based detectors. So each object query focuses on\\nthe objects near the anchor point. Moreover, our query de-\\nsign can predict multiple objects at one position to solve the\\ndifﬁculty: “one region, multiple objects”. In addition, we de-\\nsign an attention variant, which can reduce the memory cost\\nwhile achieving similar or better performance than the stan-\\ndard attention in DETR. Thanks to the query design and the\\nattention variant, the proposed detector that we called Anchor\\nDETR, can achieve better performance and run faster than\\nthe DETR with 10× fewer training epochs. For example, it\\nachieves 44.2 AP with 19 FPS on the MSCOCO dataset when\\nusing the ResNet50-DC5 feature for training 50 epochs. Ex-\\ntensive experiments on the MSCOCO benchmark prove the\\neffectiveness of the proposed methods. Code is available at\\nhttps://github.com/megvii-research/AnchorDETR.\\nIntroduction\\nThe object detection task is to predict a bounding box and a\\ncategory for each object of interest in an image. In the last\\ndecades, there are many great progresses in object detection\\nbased on the CNN (Ren et al. 2015; Cai and Vasconcelos\\n2018; Redmon et al. 2016; Lin et al. 2017; Zhang et al. 2020;\\nQiao, Chen, and Yuille 2020; Chen et al. 2021). Recently,\\nCarion et al. (Carion et al. 2020) propose the DETR which\\nis a new paradigm of object detection based on the trans-\\nformer. It uses a set of learned object queries to reason about\\nthe relations of the objects and the global image context to\\noutput the ﬁnal predictions set. However, the learned object\\nquery is very hard to explain. It does not have an explicit\\nphysical meaning and the corresponding prediction slots of\\neach object query do not have a speciﬁc mode. As shown in\\nCopyright © 2022, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.\\n(a) DETR\\n(b) Ours\\nFigure 1: Visualization of the prediction slots. Note that the\\nsub-ﬁgure (a) comes from the ﬁgure in DETR (Carion et al.\\n2020). Each prediction slot includes all box predictions on\\nthe val set of a query. Each of the colored points represents\\nthe normalized center position of a prediction. The points\\nare color-coded so that the green color corresponds to small\\nboxes, red to large horizontal boxes, and blue to large verti-\\ncal boxes. The black points in the last row of sub-ﬁgure (b)\\nindicate the anchor points. The prediction slots of ours are\\nmore related to a speciﬁc position than DETR.\\nFigure 1(a), the predictions of each object query in DETR\\nare related to different areas and each object query will be in\\ncharge of a very large area. This positional ambiguity, i.e.,\\nthe object query does not focus on a speciﬁc region, makes\\nit hard to optimize.\\nReviewing the detectors based on CNN, the anchors are\\nhighly related to the position and contain interpretable phys-\\nical meanings. Inspired by this motivation, we propose a\\nnovel query design based on the anchor points, i.e., we en-\\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\\n2567'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 1}, page_content='Feature\\nAP\\nGFLOPs FPS\\nDETR\\nDC5\\n43.3\\n187\\n12\\nSMCA\\nmulti-level 43.7\\n152\\n10\\nDeformable DETR\\nmulti-level 43.8\\n173\\n15\\nConditional DETR\\nDC5\\n43.8\\n195\\n10\\nOurs\\nDC5\\n44.2\\n172\\n19\\nTable 1: Comparison with transformer detectors. The results\\nare based on the ResNet-50 backbone and “DC5” means the\\ndilated C5 feature. The DETR is trained with 500 epochs\\nwhile the others are trained with 50 epochs. We evaluate\\nthe FPS of the proposed detector by following the script in\\nDETR and set the batch size to 1 for a fair comparison with\\nothers. Note that we follow the DETR to script the model to\\nevaluate the speed.\\ncode the anchor points as the object queries. The object\\nqueries are the encodings of anchor points’ coordinates so\\nthat each object query has an explicit physical meaning.\\nHowever, this solution will encounter difﬁculty: there will be\\nmultiple objects appearing in one position. In this situation,\\nonly one object query in this position cannot predict multi-\\nple objects, so the object queries from other positions have\\nto predict these objects collaboratively. It will cause each ob-\\nject query to be in charge of a larger area. Thus, we improve\\nthe object query design by adding multiple patterns to each\\nanchor point so that each anchor point can predict multiple\\nobjects. As shown in Figure 1(b), all the predictions of the\\nthree patterns of each object query are distributed around the\\ncorresponding anchor point. In other words, it demonstrates\\nthat each object query only focuses on the objects near the\\ncorresponding anchor point. So the proposed object query\\ncan be easy to explain. As the object query has a speciﬁc\\nmode and does not need to predict the object far away from\\nthe corresponding position, it can easier to optimize.\\nBesides the query design, we also design an attention vari-\\nant that we call Row-Column Decouple Attention (RCDA).\\nIt decouples the 2D key feature into the 1D row feature and\\nthe 1D column feature, then conducts the row attention and\\ncolumn attention successively. The RCDA can reduce the\\nmemory cost while achieving similar or better performance\\nthan the standard attention in DETR. We believe it can be a\\ngood alternative to the standard attention in DETR.\\nAs shown in Table 1, thanks to the novel query design\\nbased on the anchor point and the attention variant, the\\nproposed detector Anchor DETR can achieve better perfor-\\nmance and run faster than the original DETR with even 10×\\nfewer training epochs when using the same single-level fea-\\nture. Compared with other DETR-like detectors with 10×\\nfewer training epochs, the proposed detector achieves the\\nbest performance among them. The proposed detector can\\nachieve 44.2 AP with 19 FPS when using a single ResNet50-\\nDC5 (He et al. 2016) feature for training 50 epochs.\\nThe main contributions can be summarized as:\\n• We propose a novel query design based on anchor points\\nfor the transformer-based detectors. Moreover, we attach\\nmultiple patterns to each anchor point so that it can pre-\\ndict multiple objects for each position to deal with the\\ndifﬁculty: “one region, multiple objects”. The proposed\\nquery based on the anchor point is more explainable and\\neasier to optimize than the learned embedding. Thanks to\\nthe effectiveness of the proposed query design, our detec-\\ntor can achieve better performance with 10× fewer train-\\ning epochs than the DETR.\\n• We design an attention variant that we called Row-\\nColumn Decoupled Attention. It can reduce the memory\\ncost while achieving similar or better performance than\\nthe standard attention in DETR, which can be a good al-\\nternative to standard attention.\\n• Extensive experiments are conducted to prove the effec-\\ntiveness of each component.\\nRelated Work\\nAnchors in Object Detection\\nThere are two type of anchors used in CNN-based object\\ndetectors, i.e. anchor boxes (Ren et al. 2015; Lin et al.\\n2017) and anchor points (Tian et al. 2019; Zhou, Wang, and\\nKr¨ahenb¨uhl 2019). As the hand-craft anchor boxes need to\\nbe carefully tuned to achieve good performance, we may\\nprefer to not use the anchor boxes. We usually treat the\\nanchor-free as the anchor boxes free so that the detectors\\nusing anchor point also be treated as anchor-free (Tian et al.\\n2019; Zhou, Wang, and Kr¨ahenb¨uhl 2019). DETR (Carion\\net al. 2020) adopt neither the anchor boxes nor the anchor\\npoints. It directly predicts the absolute position of each ob-\\nject in the image. However, we ﬁnd that introducing the an-\\nchor point into the object query can be better.\\nTransformer Detector\\nCarion et al. (Carion et al. 2020) propose the DETR which\\nis based on the transformer (Vaswani et al. 2017) for ob-\\nject detection. The transformer detector will feed the in-\\nformation of value to the query based on the similarity of\\nthe query and key. Zhu et al. (Zhu et al. 2021) propose the\\nDeformable DETR that samples information of deformable\\npoints around the reference points to the query and applies\\nmultiple level features to solve the slowly converge speed\\nof the transformer detector. The reference points are simi-\\nlar to the anchor points but the object queries of it are still\\nthe learned embeddings. It predicts the reference points by\\nthe object queries rather than encoding the anchor points as\\nthe object queries. Gao et al. (Gao et al. 2021) add a Gaus-\\nsian map in the original attention for each query. Concurrent\\nwith us, the Conditional DETR (Meng et al. 2021) encodes\\nthe reference point as the query position embedding. But the\\nmotivations are different so that it only uses the reference\\npoint to generate position embedding as the conditional spa-\\ntial embedding in the cross attention and the object queries\\nare still the learned embeddings. Besides, it does not involve\\nthe multiple predictions of one position and the attention\\nvariants.\\nEfﬁcient Attention\\nThe self-attention of the transformer has high complexity so\\nit cannot well deal with a very large number of queries and\\n2568'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 2}, page_content='(a) grid anchor points\\n(b) learned anchor points\\nFigure 2: Visualization of the anchor points distribution.\\nEach point represents the normalized position of an anchor.\\nkeys. To solve this problem, many efﬁcient attention mod-\\nules have been proposed (Shen et al. 2021; Vaswani et al.\\n2017; Beltagy, Peters, and Cohan 2020; Liu et al. 2021; Ma\\net al. 2021). One method is to compute the key and value\\nﬁrst which can lead to linear complexity of the number of\\nthe query or key. The Efﬁcient Attention (Shen et al. 2021)\\nand the Linear Attention (Katharopoulos et al. 2020) follow\\nthis idea. Another method is to restrict the attention region\\nof the key for each query instead of the whole region. The\\nRestricted Self-Attention (Vaswani et al. 2017), Deformable\\nAttention (Zhu et al. 2021), Criss-Cross Attention (Huang\\net al. 2019), and LongFormer (Beltagy, Peters, and Cohan\\n2020) follow this idea. In this paper, we decouple the key\\nfeature to the row feature and the column feature by 1D\\nglobal average pooling and then perform the row attention\\nand the column attention successively.\\nMethod\\nAnchor Points\\nIn the CNN-based detectors, anchor points always are the\\ncorresponding position of the feature maps. But it can be\\nmore ﬂexible in the transformer-based detector. The anchor\\npoints can be the learned points, the uniform grid points,\\nor other hand-craft anchor points. We adopt two types of an-\\nchor points. One is the grid anchor points and the other is the\\nlearned anchor points. As shown in Figure 2(a), the gird an-\\nchor points are ﬁxed as the uniform grid points in the image.\\nThe learned points are randomly initialized with uniform\\ndistribution from 0 to 1 and updated as the learned parame-\\nters. With the anchor points, the center position ( ˆcx, ˆcy) of\\nthe predicted bounding box will be based on the correspond-\\ning anchor point like that in the Deformable DETR (Zhu\\net al. 2021).\\nAttention Formulation\\nThe attention of DETR-like transformer can be formulate as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V,\\nQ = Qf + Qp, K = Kf + Kp, V = Vf,\\n(1)\\nwhere the dk is the channel dimension, the subscript f\\nmeans the feature, and the subscript p means the position\\nembedding. The Q, K, V are the query, key, value respec-\\ntively. Note that the Q, K, V will pass through a linear layer\\nrespectively and that is omitted in Equation (1) for clarity.\\nThere are two attentions in the DETR decoder. One is\\nself-attention and the other is cross-attention. In the self-\\nattention, the Kf and the Vf are the same as the Qf while\\nthe Kp is the same as the Qp. The Qf ∈RNq×C is the out-\\nput of last decoder and the initial Qinit\\nf\\n∈RNq×C for the\\nﬁrst decoder can be set to a constant vector or learned em-\\nbedding. For the query position embedding Qp ∈RNq×C, it\\nuses a set of learned embedding in DETR:\\nQp = Embedding(Nq, C).\\n(2)\\nIn the cross-attention, the Qf ∈RNq×C is generated\\nfrom the output of the self-attention in front while the Kf ∈\\nRHW ×C and Vf ∈RHW ×C are the output feature of the en-\\ncoder. The Kp ∈RHW ×C is the position embedding of Kf.\\nIt is generated by the sine-cosine position encoding func-\\ntion (Vaswani et al. 2017; Carion et al. 2020) gsin based on\\nthe corresponding key feature position Posk ∈RHW ×2:\\nKp = gsin(Posk).\\n(3)\\nNote that the H, W, C is the height, width, channel of the\\nfeature and the Nq is the predeﬁned number of the query.\\nAnchor Points to Object Query\\nUsually, the Qp in decoder is regarded as the object query\\nbecause it is responsible for distinguishing different objects.\\nThe object query of learned embedding like Equation (2) is\\nhard to explain as discussed in the Introduction section.\\nIn this paper, we propose to design the object query based\\non the anchor point Posq. The Posq ∈RNA×2 represents\\nNA points with their (x, y) positions which range from 0\\nto 1. Then the object query based on the anchor points can\\nformulate as:\\nQp = Encode(Posq).\\n(4)\\nIt means that we encode the anchor points as the object\\nqueries.\\nSo how to design the encoding function? As the object\\nquery is designed as the query position embedding as shown\\nin Equation (1), the most natural way is to share the same\\nposition encoding function as key:\\nQp = g(Posq), Kp = g(Posk),\\n(5)\\nwhere g is the position encoding function. The position en-\\ncoding function could be the gsin or other position encoding\\nfunctions. In this paper, instead of just using the heuristic\\ngsin, we prefer to use a small MLP network with two linear\\nlayers to adapt it additionally.\\nMultiple Predictions for Each Anchor Point\\nTo deal with the situation that one position may have mul-\\ntiple objects, we further improve the object query to pre-\\ndict multiple objects for each anchor point instead of just\\none prediction. Reviewing the initial query feature Qinit\\nf\\n∈\\nRNq×C, each of the Nq object queries has one pattern Qi\\nf ∈\\nR1×C, where the i is the index of object queries. To predict\\n2569'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 3}, page_content='Encoder Layer\\nFeature\\nFeature Position\\nDecoder Layer\\nFFN\\nFFN\\nClass\\nBox \\nPattern Embeddings\\nAnchor Points\\nEncoder\\nN×\\nPosition Encoder\\nPosition Encoder\\nDecoder\\nM×\\nBackbone\\nDC5\\nF5\\n1x1 conv\\nImage\\nFigure 3: Pipeline of the proposed detector. Note that the encoder layer and decoder layer are in the same structure as DETR\\nexcept that we replace the self-attention in the encoder layer and the cross-attention in the decoder layer with the proposed\\nRow-Column Decouple Attention.\\nmultiple objects for each anchor point, we can incorporate\\nmultiple patterns into each object query. We use a small set\\npattern embedding Qi\\nf ∈RNp×C:\\nQi\\nf = Embedding(Np, C),\\n(6)\\nto detect objects with different patterns at each position. The\\nNp is the number of pattern which is very small, e.g. Np =\\n3. For the property of translation invariance, the patterns are\\nshared for all the object queries. Thus we can get the initial\\nQinit\\nf\\n∈RNpNA×C and Qp ∈RNpNA×C by sharing the\\nQi\\nf ∈RNp×C to each of the Qp ∈RNA×C. Here the Nq is\\nequal to Np ×NA. Then we can deﬁne the proposed Pattern-\\nPosition query design as:\\nQ = Qinit\\nf\\n+ Qp.\\n(7)\\nFor the following decoder, the Qf is also generated from the\\noutput of last decoder like DETR.\\nThanks to the proposed query design, the proposed de-\\ntector has an interpretable query and achieves better per-\\nformance than the original DETR with 10× fewer training\\nepochs.\\nRow-Column Decoupled Attention\\nThe transformer will cost a lot of GPU memory which may\\nlimit its use of the high-resolution feature or other exten-\\nsions. The Deformable Transformer (Zhu et al. 2021) can\\nreduce memory cost but it will lead to random access of\\nmemory which may not be friendly to modern accelerators\\nof massive parallelism. There are also some attention mod-\\nules (Ma et al. 2021; Shen et al. 2021) with linear complexity\\nand will not lead to random access of memory. However, in\\nour experiments, we ﬁnd that these attention modules cannot\\nwell deal with the DETR-like detectors. It may be because\\nthe cross-attention in the decoder is much more difﬁcult than\\nthe self-attention.\\nIn this paper, we propose the Row-Column Decoupled\\nAttention (RCDA) which can not only decrease the mem-\\nory burdens but also achieve similar or better performance\\nthan the standard attention in DETR. The main idea of the\\nRCDA is to decouple the 2D key feature Kf ∈RH×W ×C\\nto 1D row feature Kf,x ∈RW ×C and 1D column feature\\nKf,y ∈RH×C, then perform the row attention and the col-\\numn attention successively. We choose the 1D global aver-\\nage pooling to decouple the key feature by default. Without\\nlosing generality, we hypothesize W ≥H and the RCDA\\ncan be formulated as:\\nAx = softmax(QxKT\\nx\\n√dk\\n), Ax ∈RNq×W ,\\nZ = weighted sumW(Ax, V ), Z ∈RNq×H×C,\\nAy = softmax(QyKT\\ny\\n√dk\\n), Ay ∈RNq×H,\\nOut = weighted sumH(Ay, Z), Out ∈RNq×C,\\n(8)\\nwhere\\nQx = Qf + Qp,x, Qy = Qf + Qp,y,\\nQp,x = g1D(Posq,x), Qp,y = g1D(Posq,y),\\nKx = Kf,x + Kp,x, Ky = Kf,y + Kp,y,\\n(9)\\nKp,x = g1D(Posk,x), Kp,y = g1D(Posk,y),\\nV = Vf, V ∈RH×W ×C.\\nThe weighted sumW and weighted sumH operations con-\\nduct the weighted sum along the width dimension and the\\nheight dimension respectively. The Posq,x ∈RNq×1 is\\nthe corresponding row position of Qf ∈RNq×C and the\\nPosq,y ∈RNq×1, Posk,x ∈RW ×1, Posk,y ∈RH×1 are\\nin the similar manner. The g1D is the 1D position encoding\\nfunction which is similar to g and it will encode a 1D coor-\\ndinate to a vector with C channels.\\nNow we analyze why it can save the memory. We hypoth-\\nesize the head number M of multi-head attention to 1 for\\nclarity in the previous formulation without losing general-\\nity, but we should consider the head number M for mem-\\nory analysis. The major memory cost for the attention in\\n2570'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 4}, page_content='Anchor-Free NMS-Free RAM-Free Epochs\\nAP\\nAP50\\nAP75\\nAPs\\nAPm\\nAPl\\nRetinaNet\\n✓\\n36\\n38.7\\n58.0\\n41.5\\n23.3\\n42.3\\n50.3\\nFCOS\\n✓\\n✓\\n36\\n41.4\\n60.1\\n44.9\\n25.6\\n44.9\\n53.1\\nPOTO\\n✓\\n✓\\n✓\\n36\\n41.4\\n59.5\\n45.6\\n26.1\\n44.9\\n52.0\\nFaster RCNN\\n36\\n40.2\\n61.0\\n43.8\\n24.2\\n43.5\\n52.0\\nCascade RCNN\\n36\\n44.3\\n62.2\\n48.0\\n26.6\\n47.7\\n57.7\\nSparse RCNN\\n✓\\n✓\\n36\\n44.5\\n63.4\\n48.2\\n26.9\\n47.2\\n59.5\\nDETR-C5\\n✓\\n✓\\n✓\\n500\\n42.0\\n62.4\\n44.2\\n20.5\\n45.8\\n61.1\\nDETR-DC5\\n✓\\n✓\\n✓\\n500\\n43.3\\n63.1\\n45.9\\n22.5\\n47.3\\n61.1\\nSMCA\\n✓\\n✓\\n✓\\n50\\n43.7\\n63.6\\n47.2\\n24.2\\n47.0\\n60.4\\nDeformable DETR\\n✓\\n✓\\n50\\n43.8\\n62.6\\n47.7\\n26.4\\n47.1\\n58.0\\nOurs-C5\\n✓\\n✓\\n✓\\n50\\n42.1\\n63.1\\n44.9\\n22.3\\n46.2\\n60.0\\nOurs-DC5\\n✓\\n✓\\n✓\\n50\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\nDETR-C5-R101\\n✓\\n✓\\n✓\\n500\\n43.5\\n63.8\\n46.4\\n21.9\\n48.0\\n61.8\\nDETR-DC5-R101\\n✓\\n✓\\n✓\\n500\\n44.9\\n64.7\\n47.7\\n23.7\\n49.5\\n62.3\\nOurs-C5-R101\\n✓\\n✓\\n✓\\n50\\n43.5\\n64.3\\n46.6\\n23.2\\n47.7\\n61.4\\nOurs-DC5-R101\\n✓\\n✓\\n✓\\n50\\n45.1\\n65.7\\n48.8\\n25.8\\n49.4\\n61.6\\nTable 2: Comparison with other detectors. The results are based on the ResNet-50 backbone if without speciﬁc and R101\\nmeans the ResNet-101 backbone. “C5”, “DC5” indicate the detectors using a single C5 or DC5 feature respectively. The other\\ndetectors without speciﬁc use multiple-level features. The RAM means the random access of memory which usually is not\\nfriendly to hardware in practice.\\nRCDA\\nanchors\\npatterns\\nAP\\nAP50\\nAP75\\nAPs\\nAPm\\nAPl\\n39.3\\n59.4\\n41.8\\n20.7\\n42.6\\n55.0\\n✓\\n✓\\n44.2\\n65.3\\n47.2\\n24.4\\n47.8\\n61.8\\n✓\\n40.3\\n61.6\\n42.5\\n21.7\\n44.1\\n56.3\\n✓\\n✓\\n40.3\\n60.8\\n43.0\\n21.1\\n44.2\\n57.0\\n✓\\n✓\\n42.6\\n63.6\\n45.5\\n23.2\\n46.4\\n58.3\\n✓\\n✓\\n✓\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\nTable 3: Effectiveness of each component. The RCDA means to replace the standard attention with the RCDA. The “anchors”\\nmeans using the anchor points to generate the object queries. The “patterns” means assigning multiple patterns to each object\\nquery so that each position has multiple predictions.\\nDETR is the attention weight map A ∈RNq×H×W ×M. The\\nattention weight map of RCDA is Ax ∈RNq×W ×M and\\nAy ∈RNq×H×M whose memory cost is much smaller than\\nthe A. However, the major memory cost in the RCDA is the\\ntemporary result Z. So we should compare the memory cost\\nof A ∈RNq×H×W ×M and Z ∈RNq×H×C. The ratio for\\nsaving memory of the RCDA is:\\nr = (Nq × H × W × M)/(Nq × H × C)\\n= W × M/C\\n(10)\\nwhere the default setting is M = 8, C = 256. So the mem-\\nory cost is roughly the same when the large side W is equal\\nto 32 which is a typical value for the C5 feature in object de-\\ntection. When using the high-resolution feature it can save\\nthe memory, e.g saving roughly 2x memory for the C4 or\\nDC5 feature and saving 4x memory for the C3 feature.\\nExperiment\\nImplementation Details\\nWe conduct the experiments on the MS COCO (Lin et al.\\n2014) benchmark. All models are trained on the train2017\\nsplit and evaluated on the val2017. The models are trained\\non 8 GPUs with 1 image per GPU. We train our model\\non the training set for 50 epochs with the AdamW opti-\\nmizer (Loshchilov and Hutter 2019) setting the initial learn-\\ning rate to 10−5 for the backbone and 10−4 for the others.\\nThe learning rate will be decayed by a factor of 0.1 at the\\n40th epoch. We set the weight decay to 10−4 and the dropout\\nrate to 0 (i.e. remove dropout). The head for the attention is\\n8, the attention feature channel is 256 and the hidden di-\\nmension of the feed-forward network is 1024. We choose\\nthe number of anchor points to 300 and the number of pat-\\nterns to 3 by default. We use a set of learned points as anchor\\npoints by default. The number of encoder layers and decoder\\nlayers is 6 like DETR. We use the focal loss (Lin et al. 2017)\\nas the classiﬁcation loss following the Deformable DETR.\\nMain Results\\nAs shown in Table 2, we compare the proposed detec-\\ntor with RetinaNet (Lin et al. 2017), FCOS (Tian et al.\\n2019), POTO (Wang et al. 2020), Faster RCNN (Ren et al.\\n2015), Cascased RCNN (Cai and Vasconcelos 2018), Sparse\\nRCNN (Sun et al. 2020), DETR (Carion et al. 2020),\\n2571'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 5}, page_content='anchor points\\npatterns\\nAP\\nAP50\\nAP75\\nAPs\\nAPm\\nAPl\\n100\\n1\\n40.8\\n61.5\\n43.6\\n21.0\\n44.8\\n57.5\\n100\\n3\\n43.4\\n63.7\\n46.5\\n23.8\\n47.1\\n60.9\\n300\\n1\\n42.6\\n63.6\\n45.5\\n23.2\\n46.4\\n58.3\\n300\\n3\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\n900\\n1\\n42.9\\n63.8\\n46.1\\n24.4\\n46.7\\n58.9\\nTable 4: Comparison for multiple predictions of one position. We show the performance with the different number of anchor\\npoints and patterns. Note that it degenerates to the single prediction of one anchor point when the number of the pattern is 1.\\n(a)\\n(b)\\n(c)\\nFigure 4: The histograms of each pattern. We show the histograms of predicted box sizes for different patterns. The abscissa\\nrepresents the square root of the area of the predicted box with normalized width “w” and height “h”. The large boxes usually\\nappear in pattern (a), the pattern (b) focuses on the small objects, and the pattern (c) is in between.\\nAP\\nAP50\\nAP75\\nAPs\\nAPm\\nAPl\\ngrid\\n44.1\\n64.5\\n47.6\\n24.8\\n48.0\\n60.9\\nlearned\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\nTable 5: Comparison of different types of anchor points. The\\n“learned” means that it uses 300 learned anchor points. The\\n“grid” means that it uses the hand-craft grid anchor points\\nwith the number of 17 × 17 which is near to 300.\\nSMCA (Gao et al. 2021), and Deformable DETR (Zhu et al.\\n2021). The “C5” and “DC5” mean that the detector uses a\\nsingle C5 or dilated C5 feature while the other detectors use\\nmultiple-level features. Using multiple-level features usu-\\nally will have a better performance but cost more resources.\\nSurprisingly, our detector with a single DC5 feature can\\nachieve better performance than the Deformable DETR and\\nSCMA which use multiple-level features. The proposed de-\\ntector can achieve better performance than DETR with 10×\\nfewer training epochs. It proves the proposed query design\\nis very effective.\\nWe also show the property of anchor-free, NMS-free, and\\nRAM-free for each detector in Table 2. The anchor-free and\\nNMS-free indicate the detector does not need the hand-craft\\nanchor box and non-maximum suppression. The RAM-free\\nmeans the detector will not involve any random access of\\nmemory which can be very friendly to modern accelerators\\nin practice. The two-stage detectors always are not RAM-\\nfree because the region of interest (RoI) is stochastic to\\nthe hardware and the operation of the RoI-Align (He et al.\\n2017)/RoI-Pooling (Girshick 2015) will involve the random\\naccess of memory. The Deformable DETR is similar to the\\ntwo-stage detector as the position of the sample point is\\nstochastic to the hardware so that it is not RAM-free. On\\nthe contrary, the proposed detector inherits the properties of\\nanchor-free, NMS-free, and RAM-free from the DETR with\\nan improvement of performance and fewer training epochs.\\nAblation Study\\nEffectiveness of each component\\nTable 3 shows the ef-\\nfectiveness of each component that we proposed. The pro-\\nposed query design based on anchor points that can predict\\nmultiple objects for each position improves the performance\\nfrom 39.3 AP to 44.2 AP. The improvements are 4.9 AP\\nwhich proves the query design with a speciﬁc focused area\\ncan be much easier to optimize. For the attention variant\\nRCDA, it achieves similar performance as the standard at-\\ntention when adopting the proposed query design. It proves\\nthat the RCDA will not degrade the performance with lower\\nmemory cost. Besides, by applying the original query design\\nas the DETR, the RCDA can achieve 1.0 improvements. We\\nthink it is slightly easier to optimize as the attention map\\nis smaller than the standard attention. This gain will vanish\\nwhen adopting the proposed query design as the query de-\\n2572'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 6}, page_content='Attention\\nFeature\\nAP\\nAP50\\nAP75\\nAPs\\nAPm\\nAPl\\nmemory\\nLuna (Ma et al. 2021)\\nDC5\\n36.2\\n58.7\\n37.1\\n15.4\\n39.2\\n54.7\\n2.9G\\nEfﬁcient-att (Shen et al. 2021)\\n34.1\\n56.8\\n34.4\\n13.9\\n36.0\\n53.9\\n2.2G\\nstd-att\\n44.2\\n65.3\\n47.2\\n24.4\\n47.8\\n61.8\\n10.5G\\nRCDA\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\n4.4G\\nstd-att\\nC5\\n42.2\\n63.5\\n44.9\\n21.9\\n45.9\\n61.0\\n2.7G\\nRCDA\\n42.1\\n63.1\\n44.9\\n22.3\\n46.2\\n60.0\\n2.3G\\nTable 6: Comparison of different attention modules. The “std-att” means the standard attention in DETR. The linear complexity\\nattention modules can reduce memory signiﬁcantly but the performance will drop. The proposed RCDA can achieve similar\\nperformance as the standard attention in DETR while saving the memory.\\nsign has the same effect that makes it easier to optimize. The\\nobject query based on anchor points with a single prediction\\nhas 2.3 AP improvements and the multiple predictions for\\neach anchor point can have 1.6 further improvements. Ap-\\nplying the multiple predictions to the original object query\\nin DETR will not improve the performance. It is because the\\noriginal object query is not highly related to the position thus\\nit cannot get beneﬁts from the “one region, multiple predic-\\ntions”.\\nMultiple Predictions for Each Anchor Point\\nTable 4\\nshows that multiple predictions for each anchor point play\\nan essential role in query design. For a single prediction (1\\npattern), we ﬁnd that 100 anchor points are not good enough\\nand 900 anchor points get similar performance as 300 an-\\nchor points, thus we use 300 anchor points by default. The\\nmultiple predictions (3 patterns) can outperform the single\\nprediction (1 pattern) by 2.6 and 1.6 AP for 100 and 300\\nanchor points respectively. With the same number of the\\npredictions, the multiple predictions for each anchor point\\n(300 anchor points, 3 patterns) can outperform the single\\nprediction (900 anchor points, 1 pattern) for 1.3 AP. It indi-\\ncates that the improvements of multiple predictions do not\\ncome from the increased number of predictions. These re-\\nsults prove the effectiveness of multiple predictions for each\\nanchor point.\\nAnchor Points Types\\nWe have tried two types of anchor\\npoints, i.e., the grid anchor points and the learned anchor\\npoints. As shown in Figure 2, the learned anchor points dis-\\ntribute uniformly in the image which is similar to the grid\\nanchor points. We hypothesize that this is because the ob-\\njects distribute everywhere in large COCO data set. We also\\nﬁnd the grid anchor points can achieve similar performance\\nas the learned anchor points in Table 5.\\nPrediction Slots of Object Query\\nAs shown in Fig-\\nure 1(b), we can observe that the prediction slots of each\\nobject query in the proposed detector will focus on the ob-\\njects near the corresponding anchor point. As there are three\\npatterns for the anchor points, we also show the histogram of\\neach pattern in Figure 4. We ﬁnd that the patterns are related\\nto the object size as the large boxes usually appear in pattern\\n(a) while pattern (b) focuses on the small objects. But the\\nquery patterns do not just depend on the size of the object\\nbecause the small boxes can also appear in the pattern (a).\\nWe think it is because there are more small objects, and mul-\\ntiple small objects are more likely to appear in one region.\\nSo all the patterns will be in charge of the small objects.\\nRow-Column Decoupled Attention\\nAs shown in Table 6,\\nwe compare the Row-Column Decoupled Attention with\\nthe standard attention in DETR and some efﬁcient attention\\nmodules with linear complexity. The attention modules with\\nlinear complexity can signiﬁcantly reduce the training mem-\\nory compared to the standard attention module. However,\\ntheir performance drops signiﬁcantly. It may be because the\\ncross-attention in the DETR-like detectors is much more\\ndifﬁcult than the self-attention. On the contrary, the Row-\\nColumn Decoupled Attention can achieve similar perfor-\\nmance. As previously discussed, the Row-Column Decou-\\npled Attention can signiﬁcantly reduce memory when using\\nthe high-resolution feature and get roughly the same mem-\\nory cost when using the C5 feature. For example, RCDA\\nreduces the training memory from 10.5G to 4.4G and gets\\nthe same performance as the standard attention when using\\nthe DC5 feature. In conclusion, the Row-Column Decoupled\\nAttention can be efﬁcient in memory while preserving com-\\npetitive performance so that it can be a good alternative to\\nthe standard attention in DETR.\\nConclusion\\nIn this paper, we propose a detector based on the trans-\\nformer. We propose a novel query design based on the an-\\nchor point that has explicit physical meaning. The corre-\\nsponding prediction slots can have a speciﬁc mode, i.e. the\\npredictions are near the anchor point, so that it is easier\\nto optimize. Moreover, we incorporate multiple patterns to\\neach anchor point to solve the difﬁculty: “one region, multi-\\nple objects”. We also propose an attention variant named the\\nRow-Column Decoupled Attention. The Row-Column De-\\ncouple Attention can reduce the memory cost while achiev-\\ning similar or better performance than the standard attention\\nin DETR. The proposed detector can run faster and achieve\\nbetter performance with 10× fewer training epochs than the\\nDETR. The proposed detector is an end-to-end detector with\\nthe property of anchor-free, NMS-free, and RAM-free. We\\nhope the proposed detector can be useful in practice and be\\na strong simple baseline for research.\\nAcknowledgments\\nThis paper is supported by the National Key R&D Plan\\nof the Ministry of Science and Technology (Project No.\\n2573'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 7}, page_content='2020AAA0104400) and Beijing Academy of Artiﬁcial In-\\ntelligence (BAAI).\\nReferences\\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020.\\nLong-\\nformer: The long-document transformer.\\nArXiv preprint,\\nabs/2004.05150.\\nCai, Z.; and Vasconcelos, N. 2018. Cascade R-CNN: Delv-\\ning Into High Quality Object Detection. In 2018 IEEE Con-\\nference on Computer Vision and Pattern Recognition, CVPR\\n2018, Salt Lake City, UT, USA, June 18-22, 2018, 6154–\\n6162. IEEE Computer Society.\\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\\nA.; and Zagoruyko, S. 2020. End-to-end object detection\\nwith transformers. In European Conference on Computer\\nVision, 213–229. Springer.\\nChen, Q.; Wang, Y.; Yang, T.; Zhang, X.; Cheng, J.; and Sun,\\nJ. 2021. You only look one-level feature. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 13039–13048.\\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021.\\nFast Convergence of DETR with Spatially Modulated Co-\\nAttention. ArXiv preprint, abs/2101.07448.\\nGirshick, R. B. 2015. Fast R-CNN. In 2015 IEEE Interna-\\ntional Conference on Computer Vision, ICCV 2015, Santi-\\nago, Chile, December 7-13, 2015, 1440–1448. IEEE Com-\\nputer Society.\\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. B. 2017.\\nMask R-CNN. In IEEE International Conference on Com-\\nputer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,\\n2980–2988. IEEE Computer Society.\\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\\nual Learning for Image Recognition. In 2016 IEEE Confer-\\nence on Computer Vision and Pattern Recognition, CVPR\\n2016, Las Vegas, NV, USA, June 27-30, 2016, 770–778.\\nIEEE Computer Society.\\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y.; and\\nLiu, W. 2019. CCNet: Criss-Cross Attention for Semantic\\nSegmentation. In 2019 IEEE/CVF International Conference\\non Computer Vision, ICCV 2019, Seoul, Korea (South), Oc-\\ntober 27 - November 2, 2019, 603–612. IEEE.\\nKatharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret,\\nF. 2020.\\nTransformers are RNNs: Fast Autoregressive\\nTransformers with Linear Attention.\\nIn Proceedings of\\nthe 37th International Conference on Machine Learning,\\nICML 2020, 13-18 July 2020, Virtual Event, volume 119\\nof Proceedings of Machine Learning Research, 5156–5165.\\nPMLR.\\nLin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll´ar, P.\\n2017.\\nFocal Loss for Dense Object Detection.\\nIn IEEE\\nInternational Conference on Computer Vision, ICCV 2017,\\nVenice, Italy, October 22-29, 2017, 2999–3007. IEEE Com-\\nputer Society.\\nLin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\\nmanan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft\\ncoco: Common objects in context. In European conference\\non computer vision, 740–755. Springer.\\nLiu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin,\\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\\nsion transformer using shifted windows.\\nArXiv preprint,\\nabs/2103.14030.\\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\\ncay Regularization.\\nIn 7th International Conference on\\nLearning Representations, ICLR 2019, New Orleans, LA,\\nUSA, May 6-9, 2019. OpenReview.net.\\nMa, X.; Kong, X.; Wang, S.; Zhou, C.; May, J.; Ma, H.; and\\nZettlemoyer, L. 2021. Luna: Linear Uniﬁed Nested Atten-\\ntion. arXiv:2106.01540.\\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y.; Sun,\\nL.; and Wang, J. 2021. Conditional DETR for Fast Training\\nConvergence. arXiv:2108.06152.\\nQiao, S.; Chen, L.-C.; and Yuille, A. 2020. DetectoRS: De-\\ntecting Objects with Recursive Feature Pyramid and Switch-\\nable Atrous Convolution. ArXiv preprint, abs/2006.02334.\\nRedmon, J.; Divvala, S. K.; Girshick, R. B.; and Farhadi,\\nA. 2016. You Only Look Once: Uniﬁed, Real-Time Object\\nDetection. In 2016 IEEE Conference on Computer Vision\\nand Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,\\nJune 27-30, 2016, 779–788. IEEE Computer Society.\\nRen, S.; He, K.; Girshick, R. B.; and Sun, J. 2015. Faster\\nR-CNN: Towards Real-Time Object Detection with Region\\nProposal Networks. In Cortes, C.; Lawrence, N. D.; Lee,\\nD. D.; Sugiyama, M.; and Garnett, R., eds., Advances in\\nNeural Information Processing Systems 28: Annual Confer-\\nence on Neural Information Processing Systems 2015, De-\\ncember 7-12, 2015, Montreal, Quebec, Canada, 91–99.\\nShen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021. Ef-\\nﬁcient attention: Attention with linear complexities. In Pro-\\nceedings of the IEEE/CVF Winter Conference on Applica-\\ntions of Computer Vision, 3531–3539.\\nSun, P.; Zhang, R.; Jiang, Y.; Kong, T.; Xu, C.; Zhan, W.;\\nTomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2020. Sparse\\nr-cnn: End-to-end object detection with learnable proposals.\\nArXiv preprint, abs/2011.12450.\\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019.\\nFCOS:\\nFully Convolutional One-Stage Object Detection. In 2019\\nIEEE/CVF International Conference on Computer Vision,\\nICCV 2019, Seoul, Korea (South), October 27 - November\\n2, 2019, 9626–9635. IEEE.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\\ntention is All you Need.\\nIn Guyon, I.; von Luxburg, U.;\\nBengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.\\nV. N.; and Garnett, R., eds., Advances in Neural Informa-\\ntion Processing Systems 30: Annual Conference on Neural\\nInformation Processing Systems 2017, December 4-9, 2017,\\nLong Beach, CA, USA, 5998–6008.\\nWang, J.; Song, L.; Li, Z.; Sun, H.; Sun, J.; and Zheng, N.\\n2020. End-to-end object detection with fully convolutional\\nnetwork. ArXiv preprint, abs/2012.03544.\\nZhang, S.; Chi, C.; Yao, Y.; Lei, Z.; and Li, S. Z. 2020.\\nBridging the Gap Between Anchor-Based and Anchor-Free\\nDetection via Adaptive Training Sample Selection. In 2020\\n2574'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '2022-06-27T15:57:10+00:00', 'source': '../data/pdf/Object_Detection.pdf', 'file_path': '../data/pdf/Object_Detection.pdf', 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'Anchor DETR: Query Design for Transformer-Based Detector', 'author': 'Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun', 'subject': 'The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)', 'keywords': 'AAAI-22: Computer Vision (CV)', 'moddate': '2022-06-27T15:57:12+00:00', 'trapped': '', 'modDate': 'D:20220627155712Z', 'creationDate': 'D:20220627155710Z', 'page': 8}, page_content='IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19,\\n2020, 9756–9765. IEEE.\\nZhou, X.; Wang, D.; and Kr¨ahenb¨uhl, P. 2019. Objects as\\nPoints. volume abs/1904.07850.\\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\\nDeformable DETR: Deformable Transformers for End-to-\\nEnd Object Detection. In 9th International Conference on\\nLearning Representations, ICLR 2021, Virtual Event, Aus-\\ntria, May 3-7, 2021. OpenReview.net.\\n2575')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "## Load all pdf files from a directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"*.pdf\",               # pattern to match text files\n",
    "    loader_cls=PyMuPDFLoader,      # specify the loader class\n",
    "    show_progress=True,\n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loader useful link\n",
    "https://python.langchain.com/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
